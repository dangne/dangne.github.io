@inproceedings{gupta2025fastafastslowtoolpathagent,
  title={FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing}, 
  author={Advait Gupta and Rishie Raj and Dang Nguyen and Tianyi Zhou},
  year={2026},
  month={January},
  booktitle={Proceedings of the International Conference on Learning Representations},
  abbr={ICLR 2026},
  selected={false},
  url={https://arxiv.org/abs/2506.20911}, 
  pdf={https://arxiv.org/pdf/2506.20911},
  code={https://github.com/tianyi-lab/FaSTAR},
  abstract={We develop a cost-efficient neurosymbolic agent to address challenging multi-turn image editing tasks such as ``Detect the bench in the image while recoloring it to pink. Also, remove the cat for a clearer view and recolor the wall to yellow.'' It combines the fast, high-level subtask planning by large language models (LLMs) with the slow, accurate, tool-use, and local A$^*$ search per subtask to find a cost-efficient toolpath---a sequence of calls to AI tools. To save the cost of A$^*$ on similar subtasks, we perform inductive reasoning on previously successful toolpaths via LLMs to continuously extract/refine frequently used subroutines and reuse them as new tools for future tasks in an adaptive fast-slow planning, where the higher-level subroutines are explored first, and only when they fail, the low-level A$^*$ search is activated. The reusable symbolic subroutines considerably save exploration cost on the same types of subtasks applied to similar images, yielding a human-like fast-slow toolpath agent ``FaSTA$^*$'': fast subtask planning followed by rule-based subroutine selection per subtask is attempted by LLMs at first, which is expected to cover most tasks, while slow A$^*$ search is only triggered for novel and challenging subtasks. By comparing with recent image editing approaches, we demonstrate FaSTA$^*$ is significantly more computationally efficient while remaining competitive with the state-of-the-art baseline in terms of success rate.}
}

@inproceedings{balepur2025dixityescandixit,
  title={Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities}, 
  author={Nishant Balepur and Dang Nguyen and Dayeon Ki},
  year={2025},
  month={October},
  url={https://arxiv.org/abs/2510.19892}, 
  pdf={https://arxiv.org/pdf/2510.19892},
  code={https://github.com/nbalepur/dixit},
  booktitle={Wordplay @ EMNLP},
  spotlight={true},
  abbr={EMNLP 2025},
  abstract={Multi-modal large language models (MLMs) are often assessed on static, individual benchmarks -- which cannot jointly assess MLM capabilities in a single task -- or rely on human or model pairwise comparisons -- which is highly subjective, expensive, and allows models to exploit superficial shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these issues, we propose game-based evaluations to holistically assess MLM capabilities. Games require multiple abilities for players to win, are inherently competitive, and are governed by fix, objective rules, and makes evaluation more engaging, providing a robust framework to address the aforementioned challenges. We manifest this evaluation specifically through Dixit, a fantasy card game where players must generate captions for a card that trick some, but not all players, into selecting the played card. Our quantitative experiments with five MLMs show Dixit win-rate rankings are perfectly correlated with those on popular MLM benchmarks, while games between human and MLM players in Dixit reveal several differences between agent strategies and areas of improvement for MLM reasoning.},
}

@inproceedings{nguyen2025owlvizopenworldbenchmarkvisual,
  title={OWLViz: An Open-World Benchmark for Visual Question Answering}, 
  author={Thuy Nguyen and Dang Nguyen and Hoang Nguyen and Thuan Luong and Long Hoang Dang and Viet Dac Lai},
  year={2026},
  month={December},
  booktitle={Proceedings of the International Conference on Autonomous Agents and Multiagent Systems},
  abbr={AAMAS 2026},
  selected={false},
  url={https://arxiv.org/abs/2503.07631}, 
  pdf={https://arxiv.org/pdf/2503.07631},
  abstract={We present a challenging benchmark for the Open WorLd VISual question answering (OWLViz) task. OWLViz presents concise, unambiguous queries that require integrating multiple capabilities, including visual understanding, web exploration, and specialized tool usage. While humans achieve 69.2% accuracy on these intuitive tasks, even state-of-the-art VLMs struggle, with the best model, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which rely on limited vision and vision-language models as tools, perform even worse. This performance gap reveals significant limitations in multimodal systems' ability to select appropriate tools and execute complex reasoning sequences, establishing new directions for advancing practical AI research.},
  code={},
}

@inproceedings{liang2025colorbenchvlmsunderstandcolorful,
  title={ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness}, 
  author={Yijun Liang and Ming Li and Chenrui Fan and Ziyue Li and Dang Nguyen and Kwesi Cobbina and Shweta Bhardwaj and Jiuhai Chen and Fuxiao Liu and Tianyi Zhou},
  year={2025},
  month={September},
  booktitle={Proceedings of the Annual Conference on Neural Information Processing Systems},
  abbr={NeurIPS 2025},
  selected={false},
  url={https://arxiv.org/abs/2504.10514}, 
  pdf={https://arxiv.org/pdf/2504.10514},
  abstract={Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.},
  code={https://github.com/tianyi-lab/ColorBench},
}

@inproceedings{DBLP:conf/acl/NguyenCW0PHLWA025,
  author       = {Dang Nguyen and Jian Chen and Yu Wang and Gang Wu and Namyong Park and Zhengmian Hu and Hanjia Lyu and Junda Wu and Ryan Aponte and Yu Xia and Xintong Li and Jing Shi and Hongjie Chen and Viet Dac Lai and Zhouhang Xie and Sungchul Kim and Ruiyi Zhang and Tong Yu and Md. Mehrab Tanjim and Nesreen K. Ahmed and Puneet Mathur and Seunghyun Yoon and Lina Yao and Branislav Kveton and Jihyung Kil and Thien Huu Nguyen and Trung Bui and Tianyi Zhou and Ryan A. Rossi and Franck Dernoncourt},
  title        = {{GUI} Agents: {A} Survey},
  booktitle    = {Findings of the Association for Computational Linguistics},
  year         = {2025},
  month        = {May},
  abbr         = {ACL 2025},
  selected     = {true},
  url          = {https://arxiv.org/abs/2412.13501}, 
  pdf          = {https://arxiv.org/pdf/2412.13501}, 
  abstract     = {Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.},
}

@inproceedings{nguyen2024dynasaurlargelanguageagents,
  title={DynaSaur ðŸ¦–: Large Language Agents Beyond Predefined Actions}, 
  booktitle={Proceedings of the Conference on Language Modeling},
  author={Dang Nguyen and Viet Dac Lai and Seunghyun Yoon and Ryan A. Rossi and Handong Zhao and Ruiyi Zhang and Puneet Mathur and Nedim Lipka and Yu Wang and Trung Bui and Franck Dernoncourt and Tianyi Zhou},
  selected={true},
  year={2025},
  month={July},
  abbr={COLM 2025},
  pdf={https://arxiv.org/abs/2411.01747}, 
  code={https://github.com/adobe-research/dynasaur}, 
  abstract={Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly-scoped environments, we argue that it presents two major challenges when deploying LLM agents in real-world scenarios: (1) selecting from a fixed set of actions significantly restricts the planning and acting capabilities of LLM agents, and (2) this approach requires substantial human effort to enumerate and implement all possible actions, which becomes impractical in complex environments with a vast number of potential actions. In this work, we propose an LLM agent framework that enables the dynamic creation and composition of actions in an online manner. In this framework, the agent interacts with the environment by generating and executing programs written in a general-purpose programming language at each step. Furthermore, generated actions are accumulated over time for future reuse. Our extensive experiments on the GAIA benchmark demonstrate that this framework offers significantly greater flexibility and outperforms previous methods. Notably, it allows an LLM agent to recover in scenarios where no relevant action exists in the predefined set or when existing actions fail due to unforeseen edge cases. At the time of writing, we hold the top position on the GAIA public leaderboard.},
}

@inproceedings{li2024rulerimprovingllmcontrollability,
  title={RuleR: Improving LLM Controllability by Rule-based Data Recycling}, 
  author={Ming Li and Han Chen and Chenguang Wang and Dang Nguyen and Dianqi Li and Tianyi Zhou},
  booktitle={Proceedings of the Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics},
  year={2025},
  month={January},
  abbr={NAACL 2025},
  selected={false},
  pdf={https://arxiv.org/abs/2406.15938}, 
  code={https://github.com/tianyi-lab/RuleR},
  abstract={Despite the remarkable advancement of Large language models (LLMs), they still lack delicate controllability under sophisticated constraints, which is critical to enhancing their response quality and the user experience. While conditional supervised fine-tuning (SFT) can potentially improve LLM controllability, curating new SFT data to fulfill the constraints usually relies on human experts or proprietary LLMs, which is time-consuming and expensive. To bridge this gap, we propose Rule-based Data Recycling (RuleR), a human/LLM-free data augmentation method incorporating multiple constraints into the original SFT data. Instead of creating new responses from scratch, RuleR integrates linguistic or formatting rules into the original instructions and modifies the responses to fulfill the rule-defined constraints. Training on the "recycled" data consolidates LLMs capability to generate constrained outputs. Extensive experiments demonstrate RuleR's effectiveness in improving LLM controllability while maintaining general instruction-following performance.},
}

@inproceedings{nguyen-etal-2024-multi,
  title = "Multi-Objective Linguistic Control of Large Language Models",
  author = "Nguyen, Dang  and Chen, Jiuhai  and Zhou, Tianyi",
  booktitle = "Findings of the Association for Computational Linguistics",
  abstract = "Large language models (LLMs), despite their breakthroughs on many challenging benchmark tasks, prefer to generate verbose responses and lack the controllability of output complexity, which is usually preferred by human users in practice. In this paper, we study how to precisely control multiple linguistic complexities of LLM output by finetuning using off-the-shelf data. To this end, we propose multi-control tuning (MCTune), which includes multiple linguistic complexity values of ground-truth responses as controls in the input for instruction tuning. We finetune LLaMA2-7B on Alpaca-GPT4 and WizardLM datasets. Evaluations on widely used benchmarks demonstrate that our method does not only improve LLMs{'} multi-complexity controllability substantially but also retains or even enhances the quality of the responses as a side benefit.",
  selected = {false},
  url = "https://aclanthology.org/2024.findings-acl.257",
  pdf = {https://arxiv.org/abs/2406.16229},
  code = {https://github.com/tianyi-lab/mctune},
  abbr = {ACL 2024},
  year = {2024},
  month = {May},
}

@inproceedings{Huynh_Nguyen_Pham_Tran_2024,
  title={COMBAT: Alternated Training for Effective Clean-Label Backdoor Attacks},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/28019},
  abstract={Backdoor attacks pose a critical concern to the practice of using third-party data for AI development. The data can be poisoned to make a trained model misbehave when a predefined trigger pattern appears, granting the attackers illegal benefits. While most proposed backdoor attacks are dirty-label, clean-label attacks are more desirable by keeping data labels unchanged to dodge human inspection. However, designing a working clean-label attack is a challenging task, and existing clean-label attacks show underwhelming performance. In this paper, we propose a novel mechanism to develop clean-label attacks with outstanding attack performance. The key component is a trigger pattern generator, which is trained together with a surrogate model in an alternating manner. Our proposed mechanism is flexible and customizable, allowing different backdoor trigger types and behaviors for either single or multiple target labels. Our backdoor attacks can reach near-perfect attack success rates and bypass all state-of-the-art backdoor defenses, as illustrated via comprehensive experiments on standard benchmark datasets. Our code is available at https://github.com/VinAIResearch/COMBAT.},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Huynh, Tran and Nguyen, Dang and Pham, Tung and Tran, Anh},
  selected={false},
  pdf={https://ojs.aaai.org/index.php/AAAI/article/view/28019/28052},
  code={https://www.youtube.com/watch?v=dQw4w9WgXcQ},
  abbr={AAAI 2024},
  year={2024},
  month={March},
}

@inproceedings{nguyen-minh-luu-2022-textual,
  title = "Textual Manifold-based Defense Against Natural Language Adversarial Examples",
  author = "Nguyen, Dang and Luu, Anh Tuan",
  booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
  url = "https://aclanthology.org/2022.emnlp-main.443",
  abstract = "Despite the recent success of large pretrained language models in NLP, they are susceptible to adversarial examples. Concurrently, several studies on adversarial images have observed an intriguing property: the adversarial images tend to leave the low-dimensional natural data manifold. In this study, we find a similar phenomenon occurs in the contextualized embedding space of natural sentences induced by pretrained language models in which textual adversarial examples tend to have their embeddings diverge off the manifold of natural sentence embeddings. Based on this finding, we propose Textual Manifold-based Defense (TMD), a defense mechanism that learns the embedding space manifold of the underlying language model and projects novel inputs back to the approximated structure before classification. Through extensive experiments, we find that our method consistently and significantly outperforms previous defenses under various attack settings while remaining unaffected to the clean accuracy. To the best of our knowledge, this is the first kind of manifold-based defense adapted to the NLP domain.",
  pdf={https://arxiv.org/abs/2211.02878}, 
  selected={true},
  abbr={EMNLP 2022},
  code={https://github.com/dangne/tmd},
  year={2022},
  month={December},
}
