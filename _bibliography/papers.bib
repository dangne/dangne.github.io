@misc{nguyen2024guiagentssurvey,
  title={GUI Agents: A Survey}, 
  author={Dang Nguyen and Jian Chen and Yu Wang and Gang Wu and Namyong Park and Zhengmian Hu and Hanjia Lyu and Junda Wu and Ryan Aponte and Yu Xia and Xintong Li and Jing Shi and Hongjie Chen and Viet Dac Lai and Zhouhang Xie and Sungchul Kim and Ruiyi Zhang and Tong Yu and Mehrab Tanjim and Nesreen K. Ahmed and Puneet Mathur and Seunghyun Yoon and Lina Yao and Branislav Kveton and Thien Huu Nguyen and Trung Bui and Tianyi Zhou and Ryan A. Rossi and Franck Dernoncourt},
  year={2024},
  eprint={2412.13501},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  abbr={Preprint},
  selected={true},
  url={https://arxiv.org/abs/2412.13501}, 
  pdf={https://arxiv.org/pdf/2412.13501}, 
  abstract={Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.},
}

@misc{nguyen2024dynasaurlargelanguageagents,
  title={DynaSaur ðŸ¦–: Large Language Agents Beyond Predefined Actions}, 
  author={Dang Nguyen and Viet Dac Lai and Seunghyun Yoon and Ryan A. Rossi and Handong Zhao and Ruiyi Zhang and Puneet Mathur and Nedim Lipka and Yu Wang and Trung Bui and Franck Dernoncourt and Tianyi Zhou},
  eprint={2411.01747},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  selected={true},
  abbr={Preprint},
  pdf={https://arxiv.org/abs/2411.01747}, 
  code={https://github.com/adobe-research/dynasaur}, 
  abstract={Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly-scoped environments, we argue that it presents two major challenges when deploying LLM agents in real-world scenarios: (1) selecting from a fixed set of actions significantly restricts the planning and acting capabilities of LLM agents, and (2) this approach requires substantial human effort to enumerate and implement all possible actions, which becomes impractical in complex environments with a vast number of potential actions. In this work, we propose an LLM agent framework that enables the dynamic creation and composition of actions in an online manner. In this framework, the agent interacts with the environment by generating and executing programs written in a general-purpose programming language at each step. Furthermore, generated actions are accumulated over time for future reuse. Our extensive experiments on the GAIA benchmark demonstrate that this framework offers significantly greater flexibility and outperforms previous methods. Notably, it allows an LLM agent to recover in scenarios where no relevant action exists in the predefined set or when existing actions fail due to unforeseen edge cases. At the time of writing, we hold the top position on the GAIA public leaderboard.},
}

@misc{li2024rulerimprovingllmcontrollability,
  title={RuleR: Improving LLM Controllability by Rule-based Data Recycling}, 
  author={Ming Li and Han Chen and Chenguang Wang and Dang Nguyen and Dianqi Li and Tianyi Zhou},
  eprint={2406.15938},
  year={2024},
  pdf={https://arxiv.org/abs/2406.15938}, 
  abbr={NAACL 2025},
  code={https://github.com/tianyi-lab/RuleR},
  selected={true},
  abstract={Despite the remarkable advancement of Large language models (LLMs), they still lack delicate controllability under sophisticated constraints, which is critical to enhancing their response quality and the user experience. While conditional supervised fine-tuning (SFT) can potentially improve LLM controllability, curating new SFT data to fulfill the constraints usually relies on human experts or proprietary LLMs, which is time-consuming and expensive. To bridge this gap, we propose Rule-based Data Recycling (RuleR), a human/LLM-free data augmentation method incorporating multiple constraints into the original SFT data. Instead of creating new responses from scratch, RuleR integrates linguistic or formatting rules into the original instructions and modifies the responses to fulfill the rule-defined constraints. Training on the "recycled" data consolidates LLMs capability to generate constrained outputs. Extensive experiments demonstrate RuleR's effectiveness in improving LLM controllability while maintaining general instruction-following performance.},
}

@inproceedings{nguyen-etal-2024-multi,
    title = "Multi-Objective Linguistic Control of Large Language Models",
    author = "Nguyen, Dang  and
      Chen, Jiuhai  and
      Zhou, Tianyi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.257",
    doi = "10.18653/v1/2024.findings-acl.257",
    pages = "4336--4347",
    abstract = "Large language models (LLMs), despite their breakthroughs on many challenging benchmark tasks, prefer to generate verbose responses and lack the controllability of output complexity, which is usually preferred by human users in practice. In this paper, we study how to precisely control multiple linguistic complexities of LLM output by finetuning using off-the-shelf data. To this end, we propose multi-control tuning (MCTune), which includes multiple linguistic complexity values of ground-truth responses as controls in the input for instruction tuning. We finetune LLaMA2-7B on Alpaca-GPT4 and WizardLM datasets. Evaluations on widely used benchmarks demonstrate that our method does not only improve LLMs{'} multi-complexity controllability substantially but also retains or even enhances the quality of the responses as a side benefit.",
    selected = {true},
    pdf = {https://arxiv.org/abs/2406.16229},
    code = {https://github.com/tianyi-lab/mctune},
    abbr = {ACL 2024},
}

@inproceedings{Huynh_Nguyen_Pham_Tran_2024,
    title={COMBAT: Alternated Training for Effective Clean-Label Backdoor Attacks},
    volume={38},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/28019},
    DOI={10.1609/aaai.v38i3.28019},
    abstract={Backdoor attacks pose a critical concern to the practice of using third-party data for AI development. The data can be poisoned to make a trained model misbehave when a predefined trigger pattern appears, granting the attackers illegal benefits. While most proposed backdoor attacks are dirty-label, clean-label attacks are more desirable by keeping data labels unchanged to dodge human inspection. However, designing a working clean-label attack is a challenging task, and existing clean-label attacks show underwhelming performance. In this paper, we propose a novel mechanism to develop clean-label attacks with outstanding attack performance. The key component is a trigger pattern generator, which is trained together with a surrogate model in an alternating manner. Our proposed mechanism is flexible and customizable, allowing different backdoor trigger types and behaviors for either single or multiple target labels. Our backdoor attacks can reach near-perfect attack success rates and bypass all state-of-the-art backdoor defenses, as illustrated via comprehensive experiments on standard benchmark datasets. Our code is available at https://github.com/VinAIResearch/COMBAT.},
    number={3},
    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Huynh, Tran and Nguyen, Dang and Pham, Tung and Tran, Anh},
    selected={true},
    published={true},
    pdf={https://ojs.aaai.org/index.php/AAAI/article/view/28019/28052},
    code={https://www.youtube.com/watch?v=dQw4w9WgXcQ},
    abbr={AAAI 2024},
}

@inproceedings{nguyen-minh-luu-2022-textual,
    title = "Textual Manifold-based Defense Against Natural Language Adversarial Examples",
    author = "Nguyen, Dang  and
      Luu, Anh Tuan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.443",
    doi = "10.18653/v1/2022.emnlp-main.443",
    pages = "6612--6625",
    abstract = "Despite the recent success of large pretrained language models in NLP, they are susceptible to adversarial examples. Concurrently, several studies on adversarial images have observed an intriguing property: the adversarial images tend to leave the low-dimensional natural data manifold. In this study, we find a similar phenomenon occurs in the contextualized embedding space of natural sentences induced by pretrained language models in which textual adversarial examples tend to have their embeddings diverge off the manifold of natural sentence embeddings. Based on this finding, we propose Textual Manifold-based Defense (TMD), a defense mechanism that learns the embedding space manifold of the underlying language model and projects novel inputs back to the approximated structure before classification. Through extensive experiments, we find that our method consistently and significantly outperforms previous defenses under various attack settings while remaining unaffected to the clean accuracy. To the best of our knowledge, this is the first kind of manifold-based defense adapted to the NLP domain.",
    pdf={https://arxiv.org/abs/2211.02878}, 
    selected={true},
    abbr={EMNLP 2022},
    code={https://github.com/dangne/tmd},
}