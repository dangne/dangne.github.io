---
layout: about
title: about
permalink: /
subtitle: dangmn [at] umd [dot] edu # <a href='#'>Affiliations</a>. Address. Contacts. Motto. Etc.

profile:
  align: right
  image: prof_pic.png
  image_circular: false # crops the image to make it circular
  more_info: # >
    # <p>555 your office number</p>
    # <p>123 your address street</p>
    # <p>Your City, State 12345</p>

selected_papers: true # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: # leave blank to include all the news in the `_news` folder

latest_posts:
  enabled: false
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---

I am a third-year Ph.D. student in Computer Science at the [University of Maryland, College Park](https://www.cs.umd.edu/), where I am advised by Professor [Tianyi Zhou](https://tianyizhou.github.io/). Previously, I was an AI Research Resident at [VinAI Research](https://vinai.io/) (now acquired by [Qualcomm AI Research](https://www.qualcomm.com/research/artificial-intelligence/ai-residency-program)), working under the mentorship of Professor [Luu Anh Tuan](https://tuanluu.github.io/). I received my B.E. degree in Computer Engineering from the [Ho Chi Minh City University of Technology](https://hcmut.edu.vn/).

My research focuses on **Large Foundation Models (LFMs) as agents that act via programmatic actions**, often referred to as **code agents** (e.g., [DynaSaur ðŸ¦–](https://arxiv.org/abs/2411.01747) and [smolagents](https://github.com/huggingface/smolagents)). I am particularly interested in improving agent performance on real-world tasks by improving the capabilities of the underlying LFMs, rather than relying on complex agent scaffolding.

Toward this goal, my work centers around three recurring questions:

1. **Characterizing failure modes** of code agents across different base LFMs (e.g., small vs. large, open- vs. closed-source, reasoning vs. non-reasoning),
2. **Understanding why these failures occur** through the lens of language model interpretability and programmatic analysis,
3. **Developing new learning methods** to mitigate these failure modes.